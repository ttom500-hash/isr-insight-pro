import streamlit as st
import google.generativeai as genai
import google.api_core.exceptions
import tempfile
import os
import time

# --- 1. ×”×’×“×¨×ª ×“×£ ---
st.set_page_config(
    page_title="Apex Pro Enterprise",
    page_icon="ğŸ¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. ×¢×™×¦×•×‘ RTL ---
st.markdown("""
<style>
    .stApp { direction: rtl; }
    h1, h2, h3, p, div { text-align: right; }
    .stTextInput > div > div > input { text-align: right; }
    .stSelectbox > div > div > div { text-align: right; }
    .stChatMessage { direction: rtl; text-align: right; }
    .stDeployButton {display:none;}
    /* ×¢×™×¦×•×‘ ×œ×”×•×“×¢×ª ×˜×¢×™× ×” */
    .stSpinner > div {
        border-color: #0068c9 border-right-color: transparent !important;
    }
</style>
""", unsafe_allow_html=True)

# --- 3. ×›×•×ª×¨×ª ---
st.title("ğŸ¢ Apex Pro Enterprise")
st.caption("××¢×¨×›×ª × ×™×ª×•×— ×“×•×—×•×ª ×‘×–××Ÿ ×××ª")

# --- 4. ×”×’×“×¨×ª API ---
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âš ï¸ ××¤×ª×— API ×—×¡×¨ ×‘-Secrets.")
    st.stop()

# --- 5. ×”×’×“×¨×ª ×”××•×“×œ ---
model = genai.GenerativeModel(
    model_name="gemini-1.5-pro", 
    generation_config={"temperature": 0.1},
    system_instruction="××ª×” ×× ×œ×™×¡×˜ ×‘×™×˜×•×— ×‘×›×™×¨. ×”×ª××—×•×ª×š ×”×™× ×‘-IFRS 17 ×•-Solvency II. ×¢× ×” ×‘×¢×‘×¨×™×ª ××§×¦×•×¢×™×ª, ×‘×¨×•×¨×” ×•×ª××¦×™×ª×™×ª."
)

# --- 6. ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ---
def upload_to_gemini(path):
    """××¢×œ×” ×§×•×‘×¥ ×œ×’×•×’×œ ×•××—×–×™×¨ ××ª ×”××•×‘×™×™×§×˜"""
    file = genai.upload_file(path, mime_type="application/pdf")
    while file.state.name == "PROCESSING":
        time.sleep(1)
        file = genai.get_file(file.name)
    if file.state.name != "ACTIVE":
        raise Exception(f"×”×§×•×‘×¥ × ×›×©×œ ×‘×¢×™×‘×•×“: {file.state.name}")
    return file

# --- 7. ×¦×“ ×™××™×Ÿ: × ×™×”×•×œ ×§×‘×¦×™× ---
base_path = "data/Insurance_Warehouse" 

with st.sidebar:
    st.header("ğŸ—„ï¸ ××§×•×¨ ×”× ×ª×•× ×™×")
    
    mode = st.radio("×‘×—×¨ ××¦×‘:", ["××¨×›×™×•×Ÿ (GitHub)", "×”×¢×œ××” ×™×“× ×™×ª"])
    
    selected_file_path = None
    uploaded_user_file = None

    if mode == "××¨×›×™×•×Ÿ (GitHub)":
        if os.path.exists(base_path):
            companies = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
            if companies:
                col1, col2 = st.columns(2)
                with col1:
                    company = st.selectbox("×—×‘×¨×”", companies)
                with col2:
                    # ×× ×¡×” ×œ××¦×•× ×©× ×™×
                    year_path = os.path.join(base_path, company)
                    years = [d for d in os.listdir(year_path) if os.path.isdir(os.path.join(year_path, d))] if os.path.exists(year_path) else ["2025"]
                    year = st.selectbox("×©× ×”", years)
                    
                quarter = st.selectbox("×¨×‘×¢×•×Ÿ", ["Q1", "Q2", "Q3", "Q4"])
                
                # × ×ª×™×‘ ×—×™×¤×•×©
                search_path = os.path.join(base_path, company, year, quarter, "Financial_Reports")
                
                if os.path.exists(search_path):
                    files = [f for f in os.listdir(search_path) if f.endswith(".pdf")]
                    if files:
                        selected_filename = st.selectbox("×‘×—×¨ ×“×•×—", files)
                        selected_file_path = os.path.join(search_path, selected_filename)
                    else:
                        st.warning("××™×Ÿ ×§×‘×¦×™ PDF ×‘×ª×™×§×™×™×” ×–×•.")
                else:
                    st.warning("×”×ª×™×§×™×™×” ×¨×™×§×”.")
            else:
                st.warning("×”××¨×›×™×•×Ÿ ×¨×™×§.")
        else:
            st.error("×ª×™×§×™×™×ª ×”××¨×›×™×•×Ÿ ×œ× × ××¦××”.")
            
    else:
        uploaded_user_file = st.file_uploader("×’×¨×•×¨ ×œ×›××Ÿ ×“×•×— ×›×¡×¤×™", type=['pdf'])

# --- 8. ×œ×•×’×™×§×” ×¨××©×™×ª ---
final_path_to_process = selected_file_path

# ×˜×™×¤×•×œ ×‘×§×•×‘×¥ ×™×“× ×™
if uploaded_user_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_user_file.getvalue())
        final_path_to_process = tmp.name

# ×× ×’× ×•×Ÿ ×˜×¢×™× ×” ××•×˜×•××˜×™
if final_path_to_process:
    # ×‘×“×™×§×” ×× ×¦×¨×™×š ×œ×˜×¢×•×Ÿ ××—×“×©
    if "current_file_path" not in st.session_state or st.session_state.current_file_path != final_path_to_process:
        with st.spinner('××—×‘×¨ ××ª ×”×“×•×— ×œ××•×— ×©×œ Gemini...'):
            try:
                gemini_file = upload_to_gemini(final_path_to_process)
                st.session_state.gemini_file = gemini_file
                st.session_state.current_file_path = final_path_to_process
                st.session_state.chat_history = [] 
                st.toast("âœ… ×”×“×•×— ×—×•×‘×¨ ×‘×”×¦×œ×—×”!", icon="ğŸš€")
            except Exception as e:
                st.error(f"×©×’×™××” ×‘×˜×¢×™× ×”: {e}")
    
    # ×©×œ×™×¤×” ××”×–×™×›×¨×•×Ÿ
    if "gemini_file" in st.session_state:
        current_file = st.session_state.gemini_file

        # ×”×¦×’×ª ×”×™×¡×˜×•×¨×™×”
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

        for msg in st.session_state.chat_history:
            st.chat_message(msg["role"]).write(msg["content"])

        # ×§×œ×˜ ××©×ª××©
        if prompt := st.chat_input("×©××œ ××©×”×• ×¢×œ ×”×“×•×—..."):
            # ×”×¦×’×ª ×©××œ×ª ×”××©×ª××©
            st.chat_message("user").write(prompt)
            st.session_state.chat_history.append({"role": "user", "content": prompt})

            # ×ª×©×•×‘×ª ×”××•×“×œ ×‘-Streaming
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                
                try:
                    # ×”×–×¨××ª ×”×ª×©×•×‘×” ×‘×–××Ÿ ×××ª
                    response = model.generate_content([current_file, prompt], stream=True)
                    
                    for chunk in response:
                        if chunk.text:
                            full_response += chunk.text
                            # ×¢×“×›×•×Ÿ ×”×˜×§×¡×˜ ×ª×•×š ×›×“×™ ×ª× ×•×¢×”
                            message_placeholder.markdown(full_response + "â–Œ")
                    
                    # ×¡×™×•× ×•×”×¡×¨×ª ×”×¡××Ÿ ×”××”×‘×”×‘
                    message_placeholder.markdown(full_response)
                    st.session_state.chat_history.append({"role": "assistant", "content": full_response})

                except google.api_core.exceptions.NotFound:
                    st.warning("âš ï¸ ×”×§×©×¨ ×¢× ×”×§×•×‘×¥ × ×•×ª×§. ×× ×¡×” ×œ×˜×¢×•×Ÿ ××—×“×©...")
                    del st.session_state['current_file_path']
                    st.rerun()
                
                except Exception as e:
                    st.error(f"×©×’×™××”: {e}")

else:
    st.info("ğŸ‘ˆ ×‘×—×¨ ×“×•×— ×›×“×™ ×œ×”×ª×—×™×œ.")
